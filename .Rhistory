bb5_mod3 <- read.delim("bb5_mod3_values.txt",
header = FALSE, sep = "")
names(bb5_mod3) <- c("mod_val", "slope", "lower","upper")
ggplot(bb5_mod3, aes(mod_val, slope)) +
geom_line() +
geom_point() +
geom_line(aes(mod_val, lower)) +
geom_line(aes(mod_val, upper)) +
geom_ribbon(data = subset(bb5_mod3[c(1:265),]),
aes(x=mod_val, ymax=upper, ymin=lower),
alpha=0.4, fill = "blue") +
xlab("Extraversion (z-scored)") +
ylab("Direct effect of STAIS on bias") +
ggtitle("STAISxExtraversion Interaction")
ggplot(bb5_mod3, aes(mod_val, slope)) +
geom_line() +
geom_point() +
geom_line(aes(mod_val, lower)) +
geom_line(aes(mod_val, upper)) +
xlab("Extraversion (z-scored)") +
ylab("Direct effect of STAIS on bias") +
ggtitle("STAISxExtraversion Interaction")
View(bb8_mod3)
View(bb5_mod3)
bb5_mod3 <- read.delim("bb5_mod3_values.txt",
header = FALSE, sep = "")
View(bb5_mod3)
bb5_mod3 <- read.delim("bb5_mod3_values.txt",
header = FALSE, sep = "")
names(bb5_mod3) <- c("mod_val", "slope", "lower","upper")
ggplot(bb5_mod3, aes(mod_val, slope)) +
geom_line() +
geom_point() +
geom_line(aes(mod_val, lower)) +
geom_line(aes(mod_val, upper)) +
geom_ribbon(data = subset(bb5_mod3[c(1:265),]),
aes(x=mod_val, ymax=upper, ymin=lower),
alpha=0.4, fill = "blue") +
xlab("Extraversion (z-scored)") +
ylab("Direct effect of STAIS on bias") +
ggtitle("STAISxExtraversion Interaction")
### bb7 ###
bb7_mod3 <- read.delim("bb7_mod3_values.txt",
header = FALSE, sep = "")
names(bb7_mod3) <- c("mod_val", "slope", "lower","upper")
View(bb7_mod3)
ggplot(bb7_mod3, aes(mod_val, slope)) +
geom_line() +
geom_point() +
geom_line(aes(mod_val, lower)) +
geom_line(aes(mod_val, upper)) +
geom_ribbon(data = subset(bb7_mod3[c(1:256),]),
aes(x=mod_val, ymax=upper, ymin=lower),
alpha=0.4, fill = "blue") +
xlab("STAIS (z-scored)") +
ylab("Direct effect of STAIS on bias") +
ggtitle("STAISxExtraversion Interaction")
ggplot(bb7_mod3, aes(mod_val, slope)) +
geom_line() +
geom_point() +
geom_line(aes(mod_val, lower)) +
geom_line(aes(mod_val, upper)) +
geom_ribbon(data = subset(bb7_mod3[c(1:256),]),
aes(x=mod_val, ymax=upper, ymin=lower),
alpha=0.4, fill = "blue") +
geom_ribbon(data = subset(bb7_mod3[c(478:601),]),
aes(x=mod_val, ymax=upper, ymin=lower),
alpha=0.4, fill = "blue") +
xlab("STAIS (z-scored)") +
ylab("Direct effect of STAIS on bias") +
ggtitle("STAISxExtraversion Interaction")
### bb11 ###
bb11_mod3 <- read.delim("bb11_mod3_values.txt",
header = FALSE, sep = "")
names(bb11_mod3) <- c("mod_val", "slope", "lower","upper")
ggplot(bb11_mod3, aes(mod_val, slope)) +
geom_line() +
geom_point() +
geom_line(aes(mod_val, lower)) +
geom_line(aes(mod_val, upper)) +
geom_ribbon(data = subset(bb11_mod3[c(1:256),]),
aes(x=mod_val, ymax=upper, ymin=lower),
alpha=0.4, fill = "blue") +
geom_ribbon(data = subset(bb11_mod3[c(478:601),]),
aes(x=mod_val, ymax=upper, ymin=lower),
alpha=0.4, fill = "blue") +
xlab("STAIS (z-scored)") +
ylab("Direct effect of STAIS on bias") +
ggtitle("STAISxExtraversion Interaction")
View(bb11_mod3)
ggplot(bb11_mod3, aes(mod_val, slope)) +
geom_line() +
geom_point() +
geom_line(aes(mod_val, lower)) +
geom_line(aes(mod_val, upper)) +
geom_ribbon(data = subset(bb11_mod3[c(1:269),]),
aes(x=mod_val, ymax=upper, ymin=lower),
alpha=0.4, fill = "blue") +
xlab("STAIS (z-scored)") +
ylab("Direct effect of STAIS on bias") +
ggtitle("STAISxExtraversion Interaction")
setwd("~/Documents/Nick-Grad/Neta_Lab/Meta_Analysis/Moderation Analyses/RT_Analyses/")
library(utils)
library(stats)
library(tidyverse)
#install.packages("ltm")
library(ltm)
data <- read.csv("dataRT.csv")
### replace -99s with NAs ###
data[data == -99] <- NA
### Check RT and EQ correlations ###
cor.test(data$EQTotal, data$AVERAGE, use = "complete.obs", method = "spearman")
### Check RT and EQ correlations ###
cor.test(data$EQTotal, data$AVERAGE, use = "complete.obs", method = "pearson")
### Check RT and EQ correlations ###
cor.test(data$RT, data$EQCE, use = "complete.obs", method = "spearman")
cor.test(data$RT, data$EQER, use = "complete.obs", method = "spearman")
cor.test(data$RT, data$EQSS, use = "complete.obs", method = "spearman")
cor.test(data$RT, data$EQTotal, use = "complete.obs", method = "spearman")
### Check RT and EQ correlations ###
cor.test(data$RT, data$STAIS, use = "complete.obs", method = "spearman")
### Check RT and EQ correlations ###
cor.test(data$RT, data$STAIT, use = "complete.obs", method = "spearman")
data <- read.csv("dataRT.csv")
View(data)
### set wd ###
setwd("~/Documents/Nick-Grad/Neta_Lab/Meta_Analysis/Moderation Analyses/")
data <- read.csv("RT_Analyses/dataRT.csv")
long17 <- read.csv("raw_surveys/longitudinal_F17subs.csv")
long18 <- read.csv("raw_surveys/longitudinal_F18subs.csv")
View(long17)
View(long17)
long17.temp <- long17[,c("Q1":"Q10")]
IRQ <- 52:67
long17.temp <- long17[,c(IRQ)]
View(long17.temp)
View(long17)
NEO <- 108:131
long17.temp <- long17[,c(ID, IRQ, NEO)]
### read in the Fall 2017 longitudinal subjects ###
long17 <- read.csv("raw_surveys/longitudinal_F17subs.csv")
EQ <- 132:159
long17.temp <- long17[,c(ID, IRQ, NEO)]
ID <- "Q1"
EQ <- 132:159
IRQ <- 52:67
NEO <- 108:131
long17.temp <- long17[,c(ID, IRQ, NEO)]
idcol <- "Q1"
fEQ <- "Q.EQ_1"
lEQ <- "Q.EQ_28"
fIRQ <- "Q.IRQ_1"
lIRQ <- "Q.IRQ_16"
fNEO <- "Q.NEO_1"
lNEO <- "Q.NEO_24"
long17.temp <- select(long17, idcol, fEQ:lEQ,
fIRQ:lIRQ, fNEO:lNEO)
long17.temp <- select(long17, idcol, c(fEQ:lEQ,
fIRQ:lIRQ, fNEO:lNEO))
### read in the Fall 2017 longitudinal subjects ###
long17 <- read.csv("raw_surveys/longitudinal_F17subs.csv")
long17.temp <- select(long17, idcol, c(fEQ:lEQ,
fIRQ:lIRQ, fNEO:lNEO))
?select
long17.temp <- select(long17, idcol, fEQ:lEQ)
### load packages ###
library(tidyverse)
long17.temp <- select(long17, idcol, c(fEQ:lEQ,
fIRQ:lIRQ, fNEO:lNEO))
long17.temp <- select(long17, idcol, fEQ:lEQ)
View(long17)
View(long17)
View(long17)
long17.temp <- select(long17, idcol, fEQ:lEQ)
### set wd ###
### make this your working directory (file path)###
setwd("~/Documents/Nick-Grad/Neta_Lab/R01/R01_Scripts/")
### load packages ###
if(!require(tidyverse)){
install.packages("tidyverse")
library(tidyverse)
}
if(!require(psych)){
install.packages("psych")
library(psych)
}
if(!require(dplyr)){
install.packages("dplyr")
library(dplyr)
}
### import data ###
data <- read.csv("New+YA+MRI+V2_May+23,+2019_09.30.csv")
### preview data ###
View(data)
### remove junk rows ###
### typically row 1 and 2 in qualtrics csv files ###
data <- data[-(c(1,2)),]
### row names reset ###
rownames(data) <- NULL
### separate STAI from full data ###
idcol <- "Q1...ID"
firstcol <- "Q11...STAI..S_1"
lastcol <- "Q12...STAI.T_20"
STAI.data <- select(data, idcol, firstcol:lastcol)
library(MASS)
long17.temp <- select(long17, idcol, fEQ:lEQ)
long17.temp <- dplyr::select(long17, idcol, fEQ:lEQ)
idcol <- "Q1"
fEQ <- "Q.EQ_1"
lEQ <- "Q.EQ_28"
fIRQ <- "Q.IRQ_1"
lIRQ <- "Q.IRQ_16"
fNEO <- "Q.NEO_1"
lNEO <- "Q.NEO_24"
long17.temp <- dplyr::select(long17, idcol, fEQ:lEQ)
### set wd ###
setwd("~/Documents/Nick-Grad/Neta_Lab/Meta_Analysis/Moderation Analyses/")
### load packages ###
library(tidyverse)
### read RT data file ###
data <- read.csv("RT_Analyses/dataRT.csv")
### read in the Fall 2017 longitudinal subjects ###
long17 <- read.csv("raw_surveys/longitudinal_F17subs.csv")
idcol <- "Q1"
fEQ <- "Q.EQ_1"
lEQ <- "Q.EQ_28"
fIRQ <- "Q.IRQ_1"
lIRQ <- "Q.IRQ_16"
fNEO <- "Q.NEO_1"
lNEO <- "Q.NEO_24"
long17.temp <- dplyr::select(long17, idcol, fEQ:lEQ)
View(long17.temp)
long17.temp <- dplyr::select(long17, idcol, fEQ:lEQ, fIRQ:lIRQ, fNEO:lNEO)
View(long17.temp)
### read in the Fall 2018 longitudinal subjects ###
long18 <- read.csv("raw_surveys/longitudinal_F18subs.csv")
long18.temp <- dplyr::select(long18, idcol, fEQ:lEQ, fIRQ:lIRQ, fNEO:lNEO)
View(long18.temp)
View(long18.temp)
View(long17.temp)
### clean up this mess ###
### row 59 is blank ###
long17.temp <- long17.temp[-59,]
View(long17.temp)
long17.temp <- row.names(NULL)
View(long17)
long17.temp <- dplyr::select(long17, idcol, fEQ:lEQ, fIRQ:lIRQ, fNEO:lNEO)
### clean up this mess ###
### row 59 is blank ###
long17.temp <- long17.temp[-59,]
long17.temp <- row.names(long17.temp)
long17.temp <- dplyr::select(long17, idcol, fEQ:lEQ, fIRQ:lIRQ, fNEO:lNEO)
### clean up this mess ###
### row 59 is blank ###
long17.temp <- long17.temp[-59,]
View(long17.temp)
### incorrectly entered ID... fix ###
long17.temp[13,1] <-  24
View(long17.temp)
### incorrectly entered ID... fix ###
long17.temp[13,1] <-  "24"
### incorrectly entered ID... fix ###
class(long17.temp$Q1)
### incorrectly entered ID... fix ###
long17.temp$Q1 <- as.numeric(long17.temp)
### incorrectly entered ID... fix ###
long17.temp$Q1 <- as.integer(long17.temp)
### incorrectly entered ID... fix ###
long17.temp$Q1 <- as.character(long17.temp)
long17.temp <- dplyr::select(long17, idcol, fEQ:lEQ, fIRQ:lIRQ, fNEO:lNEO)
### clean up this mess ###
### row 59 is blank ###
long17.temp <- long17.temp[-59,]
### incorrectly entered ID... fix ###
long17.temp$Q1 <- as.character(long17.temp)
### incorrectly entered ID... fix ###
long17.temp$Q1 <- as.character(long17.temp$Q1)
long17.temp <- dplyr::select(long17, idcol, fEQ:lEQ, fIRQ:lIRQ, fNEO:lNEO)
### clean up this mess ###
### row 59 is blank ###
long17.temp <- long17.temp[-59,]
### incorrectly entered ID... fix ###
long17.temp$Q1 <- as.character(long17.temp$Q1)
long17.temp[13,1] <-  "24"
View(long17.temp)
long17.temp[19,1] <-  "35"
View(long17.temp)
long17.temp <- dplyr::select(long17, idcol, fEQ:lEQ, fIRQ:lIRQ, fNEO:lNEO)
### clean up this mess ###
### row 59 is blank ###
long17.temp <- long17.temp[-59,]
### incorrectly entered ID... fix ###
long17.temp$Q1 <- as.character(long17.temp$Q1)
View(long17.temp)
long17.temp[13,1] <-  "24"
long17.temp[29,1] <-  "35"
row.names(long17.temp) <- NULL
View(long17.temp)
View(long18.temp)
library(readxl)
### read in the stroop subjects ###
read_xlsx("raw_surveys/Stroop_Survey_Cleaned_20181106")
### read in the stroop subjects ###
read_xlsx("raw_surveys/Stroop_Survey_Cleaned_20181106")
### set wd ###
setwd("~/Documents/Nick-Grad/Neta_Lab/Meta_Analysis/Moderation Analyses/")
### read in the stroop subjects ###
read_xlsx("raw_surveys/Stroop_Survey_Cleaned_20181106")
### read in the stroop subjects ###
read_xlsx("/raw_surveys/Stroop_Survey_Cleaned_20181106")
### read in the stroop subjects ###
read_xlsx("raw_surveys/Stroop_Survey_Cleaned_20181106", sheet = 1)
### read in the stroop subjects ###
read_xlsx("raw_surveys/Stroop_Survey_Cleaned_20181106.xlsx", sheet = 1)
### read in the stroop subjects ###
stroop <- read_xlsx("raw_surveys/Stroop_Survey_Cleaned_20181106.xlsx", sheet = 1)
View(stroop)
idcol <- "ID-Recode"
fSTAIS <- "STAI 1 'I feel calm'"
fSTAIS <- "STAI 1 'I feel calm'"
lSTAIS <- "STAI 20 'I feel pleasant'"
stroop.temp <- dplyr::select(stroop, idcol, fSTAIS:lSTAIS)
fSTAIS <- 'STAI 1 "I feel calm"'
lSTAIS <- "STAI 20 'I feel pleasant"'
stroop.temp <- dplyr::select(stroop, idcol, fSTAIS:lSTAIS)
lSTAIS <- 'STAI 20 "I feel pleasant"'
fSTAIS <- 'STAI 1 "I feel calm"'
lSTAIS <- 'STAI 20 "I feel pleasant"'
stroop.temp <- dplyr::select(stroop, idcol, fSTAIS:lSTAIS)
View(stroop.temp)
nhpath <- '~/Documents/Nick-Grad/Neta_Lab/Words/'
path <- nhpath
setwd(path)
### load v important packages, but quietly ###
suppressPackageStartupMessages(library(plyr))
suppressPackageStartupMessages(library(readxl))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(ggplot2))
### import task data ###
## Main task, A = positive, L = negative
data1 <- read_csv('data_exp_8700-v20-9/data_exp_8700-v20_task-l2xg.csv')
## Main task, A = negative, L = positive
data2 <- read_csv('data_exp_8700-v20-9/data_exp_8700-v20_task-bx4b.csv')
## Screener task, A = positive, L = negative
data3 <- read_csv('data_exp_8700-v20-9/data_exp_8700-v20_task-jdgj.csv')
## Screener task, A = negative, L = positive
data4 <- read_csv('data_exp_8700-v20-9/data_exp_8700-v20_task-3raa.csv')
### pick the cool colomns ###
data1 <- data1[, c("Participant Public ID", "Trial Number", "Reaction Time", "Response", "Correct", "Incorrect", "randomise_trials", "display",
"ANSWER", "# exwords", "# wordlist", "Metadata")]
data2 <- data2[, c("Participant Public ID", "Trial Number", "Reaction Time", "Response", "Correct", "Incorrect", "randomise_trials", "display",
"ANSWER", "# exwords", "# wordlist", "Metadata")]
data3 <- data3[, c("Participant Public ID", "Trial Number", "Reaction Time", "Response", "Correct", "Incorrect", "randomise_trials", "display",
"ANSWER", "# exwords", "# wordlist", "Metadata")]
data4 <- data4[, c("Participant Public ID", "Trial Number", "Reaction Time", "Response", "Correct", "Incorrect", "randomise_trials", "display",
"ANSWER", "# exwords", "# wordlist", "Metadata")]
### remove the words "positive" and "negative" from the main blocks ###
data2<-data2[(data2$`# wordlist` != "POSITIVE"),]
data2<-data2[(data2$`# wordlist` != "NEGATIVE"),]
data1<-data1[(data1$`# wordlist` != "POSITIVE"),]
data1<-data1[(data1$`# wordlist` != "NEGATIVE"),]
### merge counter-balanced screening blocks ###
data <- rbind(data3, data4)
### rename (need to get rid of `#`'s)
names(data) <- c("subjID", "Trial Number", "Reaction Time", "Response", "Correct", "Incorrect", "randomise_trials", "display",
"ANSWER", "exwords", "wordlist", "Metadata")
### positive/negative were screeners and actual words of interest ###
### select only first instance of each ###
### first split data into dataframe for each subject ###
split.data <- split(data, data$subjID)
### then write over each subjects data frame with only first instance of each word ###
split.data <- lapply(split.data, function(data) {
data <- data[match(unique(data$wordlist), data$wordlist),]
})
data <- bind_rows(split.data, .id = "column_label")
data <- select(data,-c("column_label"))
### merge counter-balanced testing blocks ###
### this renaming could be made more elegant... ###
### rename (need to get rid of `#`'s)
names(data1) <- c("subjID", "Trial Number", "Reaction Time", "Response", "Correct", "Incorrect", "randomise_trials", "display",
"ANSWER", "exwords", "wordlist", "Metadata")
### rename (need to get rid of `#`'s)
names(data2) <- c("subjID", "Trial Number", "Reaction Time", "Response", "Correct", "Incorrect", "randomise_trials", "display",
"ANSWER", "exwords", "wordlist", "Metadata")
data <- rbind(data, data1, data2)
### clean workspace ###
rm(data1, data2, data3, data4)
data<-data[!is.na(data$wordlist),]
### check list of participants ###
participants <- unique(data$subjID)
### count # of trials per participant ###
pay <- plyr::count(data$subjID)
pay$x <- as.character(pay$x)
final.participant <- ifelse((pay$freq>629), pay$x, NA)
final.participant <- na.omit(final.participant)
data <- data[ data$subjID %in% final.participant, ]
### create 1 = neg and 0 = pos score for each trial ###
data$rating <- ifelse(data$Response == "negative", 1,
ifelse(data$Response == "positive", 0, NA))
### insane was doubled... oops, select only first instance ###
### first split data into dataframe for each subject ###
split.data <- split(data, data$subjID)
### then write over each subjects data frame with only first instance of each word ###
split.data <- lapply(split.data, function(data) {
data <- data[match(unique(data$wordlist), data$wordlist),]
})
### marry the data again ###
data <- bind_rows(split.data, .id = "column_label")
### use to swtich b/w different RT cutoffs ###
###                                        ###
data <- subset(data, (`Reaction Time` >= 250 & `Reaction Time` <= 2932))
### remove bad subjects (i.e., A1DCKRRPA4AWVD) ###
data <- subset(data, !subjID == "A1DCKRRPA4AWVD")
### grab mean and standard deviation of postiive/negative judgments ###
words.summary <- (ddply(data, "wordlist", plyr::summarise,
neg.avg = mean(rating, na.rm = FALSE),
neg.sd = sd(rating, na.rm = FALSE),
RT = mean(`Reaction Time`, na.rm = FALSE),
RT.sd = sd(`Reaction Time`, na.rm = FALSE),
avg.cor = mean(Correct, na.rm = FALSE),
avg.inc = mean(Incorrect, na.rm = FALSE)))
source("data_cleaning_nh.R")
nhpath <- '~/Documents/Nick-Grad/Neta_Lab/Words/'
path <- nhpath
setwd(path)
library(openxlsx)
### plot the words ###
ggplot(aes(x = wordlist, y = neg.avg), data = words.summary) +
geom_point(stat = "identity")
### plot all RTs per subj ###
data$`Participant Public ID` <- as.character(data$`Participant Public ID`)
source("data_cleaning_nh.R")
### plot all RTs per subj ###
data$`Participant Public ID` <- as.character(data$`Participant Public ID`)
### plot all RTs per subj ###
data$`subjID` <- as.character(data$`subjID`)
ggplot(data = data, aes(x = `subjID`, y = `Reaction Time`)) +
geom_point() +
ylim(0, 10000) +
geom_hline(mapping = NULL, yintercept = 2932.776)
### calculate percent trials retained after RT cutoff ###
orig <- data %>% count(`subjID`)
### plot the words between .25 and .75 neg.avg ###
ggplot(aes(x = wordlist, y = RT, color = Val), data = subset(words.summary, (words.summary$neg.avg >= .75))) +
geom_point(stat = "identity")
### plot the words between .25 and .75 neg.avg ###
ggplot(aes(x = wordlist, y = RT, color = Val), data = subset(words.summary, (words.summary$neg.avg <= .75 & words.summary$neg.avg >= .25))) +
geom_point(stat = "identity")
source("data_cleaning_nh.R")
source("data_cleaning_nh.R")
### plot all RTs per subj ###
data$`subjID` <- as.character(data$`subjID`)
ggplot(data = data, aes(x = `subjID`, y = `Reaction Time`)) +
geom_point() +
ylim(0, 10000) +
geom_hline(mapping = NULL, yintercept = 2932.776)
source("data_cleaning_nh.R")
### plot all RTs per subj ###
data$`Participant Public ID` <- as.character(data$`Participant Public ID`)
ggplot(data = data, aes(x = `Participant Public ID`, y = `Reaction Time`)) +
geom_point() +
ylim(0, 10000) +
geom_hline(mapping = NULL, yintercept = 2932.776)
source("data_cleaning_nh.R")
### plot all RTs per subj ###
data$`Participant Public ID` <- as.character(data$`Participant Public ID`)
ggplot(data = data, aes(x = `Participant Public ID`, y = `Reaction Time`)) +
geom_point() +
ylim(0, 10000) +
geom_hline(mapping = NULL, yintercept = 2932.776)
sub <- subset(data, (`Reaction Time` >= 250 & `Reaction Time` <= 2214.349)) %>% count(`Participant Public ID`)
sub <- subset(data, (`Reaction Time` >= 250 & `Reaction Time` <= 2932.776)) %>% count(`Participant Public ID`)
comb <- merge(orig, sub, by = "Participant Public ID")
### looks like 21 positive (e.g., below .25) and 20 negative (e.g., above .75) ###
pos.words <- subset(words.summary, words.summary$neg.avg <= .25)
neg.words <- subset(words.summary, words.summary$neg.avg >= .75)
source("data_cleaning_nh.R")
### looks like 21 positive (e.g., below .25) and 20 negative (e.g., above .75) ###
pos.words <- subset(words.summary, words.summary$neg.avg <= .25)
neg.words <- subset(words.summary, words.summary$neg.avg >= .75)
### plot the words ###
ggplot(aes(x = wordlist, y = neg.avg), data = words.summary) +
geom_point(stat = "identity")
### looks like 20 positive (e.g., below .25) and 19 negative (e.g., above .75) ###
pos.words <- subset(words.summary, words.summary$neg.avg <= .25)
neg.words <- subset(words.summary, words.summary$neg.avg >= .75)
### plot the words between .25 and .75 neg.avg ###
ggplot(aes(x = wordlist, y = RT, color = Val), data = subset(words.summary, (words.summary$neg.avg <= .75 & words.summary$neg.avg >= .25))) +
geom_point(stat = "identity")
source("data_cleaning_nh.R")
mean(data$`Reaction Time`)
sd(data$`Reaction Time`)
777.4961 + (3 * 718.4265)
source("data_cleaning_nh.R")
amb.words <- subset(words.summary, (words.summary$neg.avg <= .75 & words.summary$neg.avg >= .25))
amb.words <- subset(amb.words, RT >= 875)
View(amb.words)
source("data_cleaning_nh.R")
data2 <- subset(data, (`Reaction Time` >= 250 & `Reaction Time` <= 3000))
### pull words and list for pos, neg, and amb ###
### want quick pos and neg words ###
### want slow amb words ###
pos.words <- subset(pos.words, RT <= 800)
source("data_cleaning_nh.R")
### looks like 20 positive (e.g., below .25) and 19 negative (e.g., above .75) ###
pos.words <- subset(words.summary, words.summary$neg.avg <= .25)
neg.words <- subset(words.summary, words.summary$neg.avg >= .75)
### pull words and list for pos, neg, and amb ###
### want quick pos and neg words ###
### want slow amb words ###
pos.words <- subset(pos.words, RT <= 800)
neg.words <- subset(neg.words, RT <= 800)
